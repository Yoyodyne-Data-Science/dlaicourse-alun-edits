{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zX4Kg8DUTKWO"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRnDnCW-Z7qv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soPGVheskaQP"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJtwVB2NbOAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "66\n",
      "8\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['in'])\n",
    "print(tokenizer.word_index['the'])\n",
    "print(tokenizer.word_index['town'])\n",
    "print(tokenizer.word_index['of'])\n",
    "print(tokenizer.word_index['athy'])\n",
    "print(tokenizer.word_index['one'])\n",
    "print(tokenizer.word_index['jeremy'])\n",
    "print(tokenizer.word_index['lanigan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49Cv68JOakwv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  4  2 66  8 67 68 69]\n"
     ]
    }
   ],
   "source": [
    "print(xs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY-jwvfgbEF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(ys[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtzlUMYadhKt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  4  2 66  8 67 68]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(xs[5])\n",
    "print(ys[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4myRpB1c4Gg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Temp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 453 samples\n",
      "Epoch 1/500\n",
      "453/453 [==============================] - 2s 4ms/sample - loss: 5.5696 - accuracy: 0.0287\n",
      "Epoch 2/500\n",
      "453/453 [==============================] - 0s 294us/sample - loss: 5.5481 - accuracy: 0.0508\n",
      "Epoch 3/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 5.4974 - accuracy: 0.0508\n",
      "Epoch 4/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 5.3384 - accuracy: 0.0508\n",
      "Epoch 5/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 5.1388 - accuracy: 0.0508\n",
      "Epoch 6/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 5.0770 - accuracy: 0.0508\n",
      "Epoch 7/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 5.0338 - accuracy: 0.0508\n",
      "Epoch 8/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 5.0008 - accuracy: 0.0574\n",
      "Epoch 9/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 4.9702 - accuracy: 0.0662\n",
      "Epoch 10/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 4.9367 - accuracy: 0.0574\n",
      "Epoch 11/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 4.9060 - accuracy: 0.0464\n",
      "Epoch 12/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 4.8731 - accuracy: 0.0552\n",
      "Epoch 13/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 4.8371 - accuracy: 0.0640\n",
      "Epoch 14/500\n",
      "453/453 [==============================] - 0s 352us/sample - loss: 4.8017 - accuracy: 0.0596\n",
      "Epoch 15/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 4.7645 - accuracy: 0.0618\n",
      "Epoch 16/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 4.7232 - accuracy: 0.0684\n",
      "Epoch 17/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 4.6839 - accuracy: 0.0684\n",
      "Epoch 18/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 4.6400 - accuracy: 0.0706\n",
      "Epoch 19/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 4.5926 - accuracy: 0.0662\n",
      "Epoch 20/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 4.5542 - accuracy: 0.0728\n",
      "Epoch 21/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 4.5081 - accuracy: 0.0662\n",
      "Epoch 22/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 4.4681 - accuracy: 0.0817\n",
      "Epoch 23/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 4.4222 - accuracy: 0.0839\n",
      "Epoch 24/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 4.3842 - accuracy: 0.0861\n",
      "Epoch 25/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 4.3387 - accuracy: 0.1038\n",
      "Epoch 26/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 4.2890 - accuracy: 0.1038\n",
      "Epoch 27/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 4.2509 - accuracy: 0.1258\n",
      "Epoch 28/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 4.2007 - accuracy: 0.1214\n",
      "Epoch 29/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 4.1503 - accuracy: 0.1413\n",
      "Epoch 30/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 4.1059 - accuracy: 0.1457\n",
      "Epoch 31/500\n",
      "453/453 [==============================] - 0s 350us/sample - loss: 4.0598 - accuracy: 0.1479\n",
      "Epoch 32/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 4.0076 - accuracy: 0.1545\n",
      "Epoch 33/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 3.9686 - accuracy: 0.1656\n",
      "Epoch 34/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 3.9071 - accuracy: 0.1788\n",
      "Epoch 35/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 3.8674 - accuracy: 0.1766\n",
      "Epoch 36/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 3.8115 - accuracy: 0.1921\n",
      "Epoch 37/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 3.7634 - accuracy: 0.2097\n",
      "Epoch 38/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 3.7210 - accuracy: 0.2163\n",
      "Epoch 39/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 3.6774 - accuracy: 0.2185\n",
      "Epoch 40/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 3.6265 - accuracy: 0.2252\n",
      "Epoch 41/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 3.5846 - accuracy: 0.2450\n",
      "Epoch 42/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 3.5652 - accuracy: 0.2517\n",
      "Epoch 43/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 3.5245 - accuracy: 0.2384\n",
      "Epoch 44/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 3.4681 - accuracy: 0.2671\n",
      "Epoch 45/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 3.4063 - accuracy: 0.2848\n",
      "Epoch 46/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 3.3672 - accuracy: 0.2826\n",
      "Epoch 47/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 3.3174 - accuracy: 0.2958\n",
      "Epoch 48/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 3.2739 - accuracy: 0.3179\n",
      "Epoch 49/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 3.2295 - accuracy: 0.3400\n",
      "Epoch 50/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 3.1823 - accuracy: 0.3664\n",
      "Epoch 51/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 3.1488 - accuracy: 0.3731\n",
      "Epoch 52/500\n",
      "453/453 [==============================] - 0s 373us/sample - loss: 3.1232 - accuracy: 0.3576\n",
      "Epoch 53/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 3.0846 - accuracy: 0.3841\n",
      "Epoch 54/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 3.0468 - accuracy: 0.3863\n",
      "Epoch 55/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 3.0064 - accuracy: 0.4018\n",
      "Epoch 56/500\n",
      "453/453 [==============================] - 0s 375us/sample - loss: 2.9732 - accuracy: 0.3974\n",
      "Epoch 57/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 2.9253 - accuracy: 0.4305\n",
      "Epoch 58/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 2.8891 - accuracy: 0.4305\n",
      "Epoch 59/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 2.8464 - accuracy: 0.4415\n",
      "Epoch 60/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 2.8154 - accuracy: 0.4415\n",
      "Epoch 61/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 2.7733 - accuracy: 0.4481\n",
      "Epoch 62/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 2.7457 - accuracy: 0.4481\n",
      "Epoch 63/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 2.7111 - accuracy: 0.4945\n",
      "Epoch 64/500\n",
      "453/453 [==============================] - 0s 350us/sample - loss: 2.6653 - accuracy: 0.4812\n",
      "Epoch 65/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 2.6267 - accuracy: 0.5033\n",
      "Epoch 66/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 2.6029 - accuracy: 0.5033\n",
      "Epoch 67/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 2.5748 - accuracy: 0.5298\n",
      "Epoch 68/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 2.5455 - accuracy: 0.5166\n",
      "Epoch 69/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 2.5133 - accuracy: 0.5276\n",
      "Epoch 70/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 2.4758 - accuracy: 0.5386\n",
      "Epoch 71/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 2.4342 - accuracy: 0.5497\n",
      "Epoch 72/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 2.4011 - accuracy: 0.5651\n",
      "Epoch 73/500\n",
      "453/453 [==============================] - 0s 386us/sample - loss: 2.3702 - accuracy: 0.5695\n",
      "Epoch 74/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 2.3435 - accuracy: 0.5850\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 341us/sample - loss: 2.3055 - accuracy: 0.6093\n",
      "Epoch 76/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 2.2810 - accuracy: 0.6004\n",
      "Epoch 77/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 2.2579 - accuracy: 0.6115\n",
      "Epoch 78/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 2.2297 - accuracy: 0.6203\n",
      "Epoch 79/500\n",
      "453/453 [==============================] - 0s 321us/sample - loss: 2.2001 - accuracy: 0.6269\n",
      "Epoch 80/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 2.1742 - accuracy: 0.6269\n",
      "Epoch 81/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 2.1799 - accuracy: 0.6313\n",
      "Epoch 82/500\n",
      "453/453 [==============================] - 0s 324us/sample - loss: 2.1513 - accuracy: 0.6137\n",
      "Epoch 83/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 2.1151 - accuracy: 0.6313\n",
      "Epoch 84/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 2.0940 - accuracy: 0.6402\n",
      "Epoch 85/500\n",
      "453/453 [==============================] - 0s 318us/sample - loss: 2.0595 - accuracy: 0.6446\n",
      "Epoch 86/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 2.0250 - accuracy: 0.6667\n",
      "Epoch 87/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 1.9929 - accuracy: 0.6689\n",
      "Epoch 88/500\n",
      "453/453 [==============================] - 0s 320us/sample - loss: 1.9619 - accuracy: 0.6821\n",
      "Epoch 89/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 1.9422 - accuracy: 0.6667\n",
      "Epoch 90/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 1.9129 - accuracy: 0.6843\n",
      "Epoch 91/500\n",
      "453/453 [==============================] - 0s 323us/sample - loss: 1.8890 - accuracy: 0.6954\n",
      "Epoch 92/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 1.8695 - accuracy: 0.6909\n",
      "Epoch 93/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 1.8520 - accuracy: 0.7086\n",
      "Epoch 94/500\n",
      "453/453 [==============================] - 0s 311us/sample - loss: 1.8329 - accuracy: 0.7042\n",
      "Epoch 95/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 1.8106 - accuracy: 0.7086\n",
      "Epoch 96/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 1.8024 - accuracy: 0.7219\n",
      "Epoch 97/500\n",
      "453/453 [==============================] - 0s 377us/sample - loss: 1.7643 - accuracy: 0.7174\n",
      "Epoch 98/500\n",
      "453/453 [==============================] - 0s 322us/sample - loss: 1.8263 - accuracy: 0.6821\n",
      "Epoch 99/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 1.7692 - accuracy: 0.7064\n",
      "Epoch 100/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 1.7330 - accuracy: 0.7020\n",
      "Epoch 101/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 1.7210 - accuracy: 0.7130\n",
      "Epoch 102/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 1.6872 - accuracy: 0.7329\n",
      "Epoch 103/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 1.6547 - accuracy: 0.7373\n",
      "Epoch 104/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 1.6391 - accuracy: 0.7417\n",
      "Epoch 105/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 1.6090 - accuracy: 0.7461\n",
      "Epoch 106/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 1.5899 - accuracy: 0.7616\n",
      "Epoch 107/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 1.5685 - accuracy: 0.7572\n",
      "Epoch 108/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 1.5511 - accuracy: 0.7682\n",
      "Epoch 109/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 1.5366 - accuracy: 0.7682\n",
      "Epoch 110/500\n",
      "453/453 [==============================] - 0s 321us/sample - loss: 1.5147 - accuracy: 0.7792\n",
      "Epoch 111/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 1.5124 - accuracy: 0.7528\n",
      "Epoch 112/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 1.4823 - accuracy: 0.7815\n",
      "Epoch 113/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 1.4593 - accuracy: 0.7903\n",
      "Epoch 114/500\n",
      "453/453 [==============================] - 0s 321us/sample - loss: 1.4412 - accuracy: 0.7947\n",
      "Epoch 115/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 1.4262 - accuracy: 0.7969\n",
      "Epoch 116/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 1.4083 - accuracy: 0.7903\n",
      "Epoch 117/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 1.3893 - accuracy: 0.7969\n",
      "Epoch 118/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 1.3818 - accuracy: 0.7991\n",
      "Epoch 119/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 1.3599 - accuracy: 0.8035\n",
      "Epoch 120/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 1.3510 - accuracy: 0.7969\n",
      "Epoch 121/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 1.3263 - accuracy: 0.8057\n",
      "Epoch 122/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 1.3117 - accuracy: 0.8168\n",
      "Epoch 123/500\n",
      "453/453 [==============================] - 0s 320us/sample - loss: 1.2990 - accuracy: 0.8146\n",
      "Epoch 124/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 1.2799 - accuracy: 0.8212\n",
      "Epoch 125/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 1.2643 - accuracy: 0.8256\n",
      "Epoch 126/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 1.2531 - accuracy: 0.8300\n",
      "Epoch 127/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 1.2403 - accuracy: 0.8234\n",
      "Epoch 128/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 1.2253 - accuracy: 0.8411\n",
      "Epoch 129/500\n",
      "453/453 [==============================] - 0s 320us/sample - loss: 1.2089 - accuracy: 0.8433\n",
      "Epoch 130/500\n",
      "453/453 [==============================] - ETA: 0s - loss: 1.2078 - accuracy: 0.83 - 0s 342us/sample - loss: 1.1938 - accuracy: 0.8389\n",
      "Epoch 131/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 1.1809 - accuracy: 0.8389\n",
      "Epoch 132/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 1.1685 - accuracy: 0.8411\n",
      "Epoch 133/500\n",
      "453/453 [==============================] - 0s 350us/sample - loss: 1.1540 - accuracy: 0.8455\n",
      "Epoch 134/500\n",
      "453/453 [==============================] - 0s 371us/sample - loss: 1.1444 - accuracy: 0.8477\n",
      "Epoch 135/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 1.1300 - accuracy: 0.8521\n",
      "Epoch 136/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 1.1171 - accuracy: 0.8477\n",
      "Epoch 137/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 1.1121 - accuracy: 0.8477\n",
      "Epoch 138/500\n",
      "453/453 [==============================] - 0s 353us/sample - loss: 1.1087 - accuracy: 0.8411\n",
      "Epoch 139/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 1.0925 - accuracy: 0.8609\n",
      "Epoch 140/500\n",
      "453/453 [==============================] - 0s 325us/sample - loss: 1.0872 - accuracy: 0.8565\n",
      "Epoch 141/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 1.0687 - accuracy: 0.8565\n",
      "Epoch 142/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 1.0552 - accuracy: 0.8631\n",
      "Epoch 143/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 1.0405 - accuracy: 0.8675\n",
      "Epoch 144/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 1.0422 - accuracy: 0.8565\n",
      "Epoch 145/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 1.0237 - accuracy: 0.8720\n",
      "Epoch 146/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 1.0014 - accuracy: 0.8675\n",
      "Epoch 147/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 0.9898 - accuracy: 0.8720\n",
      "Epoch 148/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 0.9906 - accuracy: 0.8852\n",
      "Epoch 149/500\n",
      "453/453 [==============================] - 0s 322us/sample - loss: 0.9930 - accuracy: 0.8808\n",
      "Epoch 150/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 0.9761 - accuracy: 0.8830\n",
      "Epoch 151/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 336us/sample - loss: 0.9650 - accuracy: 0.8786\n",
      "Epoch 152/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.9485 - accuracy: 0.8830\n",
      "Epoch 153/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 0.9375 - accuracy: 0.8962\n",
      "Epoch 154/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 0.9203 - accuracy: 0.8874\n",
      "Epoch 155/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.9248 - accuracy: 0.8962\n",
      "Epoch 156/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 0.9397 - accuracy: 0.8852\n",
      "Epoch 157/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.9123 - accuracy: 0.8918\n",
      "Epoch 158/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.9130 - accuracy: 0.8830\n",
      "Epoch 159/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.8975 - accuracy: 0.8874\n",
      "Epoch 160/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.8810 - accuracy: 0.8852\n",
      "Epoch 161/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 0.8745 - accuracy: 0.8830\n",
      "Epoch 162/500\n",
      "453/453 [==============================] - 0s 320us/sample - loss: 0.8768 - accuracy: 0.8808\n",
      "Epoch 163/500\n",
      "453/453 [==============================] - 0s 318us/sample - loss: 0.8602 - accuracy: 0.8852\n",
      "Epoch 164/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.8399 - accuracy: 0.9051\n",
      "Epoch 165/500\n",
      "453/453 [==============================] - 0s 324us/sample - loss: 0.8239 - accuracy: 0.9007\n",
      "Epoch 166/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.8154 - accuracy: 0.9029\n",
      "Epoch 167/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 0.8166 - accuracy: 0.8940\n",
      "Epoch 168/500\n",
      "453/453 [==============================] - 0s 322us/sample - loss: 0.8475 - accuracy: 0.8918\n",
      "Epoch 169/500\n",
      "453/453 [==============================] - 0s 352us/sample - loss: 0.8107 - accuracy: 0.9007\n",
      "Epoch 170/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.8015 - accuracy: 0.9051\n",
      "Epoch 171/500\n",
      "453/453 [==============================] - 0s 311us/sample - loss: 0.7830 - accuracy: 0.9117\n",
      "Epoch 172/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 0.7760 - accuracy: 0.9161\n",
      "Epoch 173/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.7701 - accuracy: 0.9095\n",
      "Epoch 174/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 0.7652 - accuracy: 0.9117\n",
      "Epoch 175/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.7510 - accuracy: 0.9139\n",
      "Epoch 176/500\n",
      "453/453 [==============================] - 0s 323us/sample - loss: 0.7395 - accuracy: 0.9161\n",
      "Epoch 177/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.7287 - accuracy: 0.9205\n",
      "Epoch 178/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 0.7216 - accuracy: 0.9272\n",
      "Epoch 179/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.7062 - accuracy: 0.9272\n",
      "Epoch 180/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.6966 - accuracy: 0.9249\n",
      "Epoch 181/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.6905 - accuracy: 0.9272\n",
      "Epoch 182/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.6833 - accuracy: 0.9249\n",
      "Epoch 183/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.6769 - accuracy: 0.9227\n",
      "Epoch 184/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 0.6745 - accuracy: 0.9249\n",
      "Epoch 185/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 0.6629 - accuracy: 0.9249\n",
      "Epoch 186/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.6535 - accuracy: 0.9272\n",
      "Epoch 187/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 0.6450 - accuracy: 0.9316\n",
      "Epoch 188/500\n",
      "453/453 [==============================] - 0s 325us/sample - loss: 0.6407 - accuracy: 0.9294\n",
      "Epoch 189/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.6317 - accuracy: 0.9294\n",
      "Epoch 190/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 0.6296 - accuracy: 0.9338\n",
      "Epoch 191/500\n",
      "453/453 [==============================] - 0s 319us/sample - loss: 0.6218 - accuracy: 0.9294\n",
      "Epoch 192/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 0.6202 - accuracy: 0.9294\n",
      "Epoch 193/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 0.6145 - accuracy: 0.9294\n",
      "Epoch 194/500\n",
      "453/453 [==============================] - 0s 313us/sample - loss: 0.6034 - accuracy: 0.9382\n",
      "Epoch 195/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.5974 - accuracy: 0.9360\n",
      "Epoch 196/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.5877 - accuracy: 0.9426\n",
      "Epoch 197/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.5811 - accuracy: 0.9404\n",
      "Epoch 198/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.5750 - accuracy: 0.9448\n",
      "Epoch 199/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 0.5739 - accuracy: 0.9426\n",
      "Epoch 200/500\n",
      "453/453 [==============================] - 0s 338us/sample - loss: 0.5761 - accuracy: 0.9382\n",
      "Epoch 201/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.6066 - accuracy: 0.9272\n",
      "Epoch 202/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.5708 - accuracy: 0.9426\n",
      "Epoch 203/500\n",
      "453/453 [==============================] - 0s 311us/sample - loss: 0.5668 - accuracy: 0.9382\n",
      "Epoch 204/500\n",
      "453/453 [==============================] - 0s 320us/sample - loss: 0.5559 - accuracy: 0.9382\n",
      "Epoch 205/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 0.5450 - accuracy: 0.9404\n",
      "Epoch 206/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.5350 - accuracy: 0.9404\n",
      "Epoch 207/500\n",
      "453/453 [==============================] - 0s 318us/sample - loss: 0.5293 - accuracy: 0.9404\n",
      "Epoch 208/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 0.5266 - accuracy: 0.9404\n",
      "Epoch 209/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.5191 - accuracy: 0.9426\n",
      "Epoch 210/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 0.5133 - accuracy: 0.9426\n",
      "Epoch 211/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.5091 - accuracy: 0.9382\n",
      "Epoch 212/500\n",
      "453/453 [==============================] - 0s 308us/sample - loss: 0.5027 - accuracy: 0.9404\n",
      "Epoch 213/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 0.4996 - accuracy: 0.9426\n",
      "Epoch 214/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.4936 - accuracy: 0.9426\n",
      "Epoch 215/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 0.4890 - accuracy: 0.9404\n",
      "Epoch 216/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 0.4838 - accuracy: 0.9404\n",
      "Epoch 217/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 0.4799 - accuracy: 0.9448\n",
      "Epoch 218/500\n",
      "453/453 [==============================] - 0s 383us/sample - loss: 0.4750 - accuracy: 0.9426\n",
      "Epoch 219/500\n",
      "453/453 [==============================] - 0s 397us/sample - loss: 0.4721 - accuracy: 0.9426 - loss: 0.4498 - accuracy: 0.\n",
      "Epoch 220/500\n",
      "453/453 [==============================] - 0s 391us/sample - loss: 0.4658 - accuracy: 0.9426\n",
      "Epoch 221/500\n",
      "453/453 [==============================] - 0s 412us/sample - loss: 0.4618 - accuracy: 0.9426\n",
      "Epoch 222/500\n",
      "453/453 [==============================] - 0s 423us/sample - loss: 0.4565 - accuracy: 0.9426\n",
      "Epoch 223/500\n",
      "453/453 [==============================] - 0s 376us/sample - loss: 0.4563 - accuracy: 0.9426\n",
      "Epoch 224/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 0.4523 - accuracy: 0.9426\n",
      "Epoch 225/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.4499 - accuracy: 0.9448\n",
      "Epoch 226/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 0.4444 - accuracy: 0.9426\n",
      "Epoch 227/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 322us/sample - loss: 0.4447 - accuracy: 0.9404\n",
      "Epoch 228/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.4377 - accuracy: 0.9404\n",
      "Epoch 229/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 0.4324 - accuracy: 0.9448\n",
      "Epoch 230/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.4284 - accuracy: 0.9426\n",
      "Epoch 231/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 0.4230 - accuracy: 0.9426\n",
      "Epoch 232/500\n",
      "453/453 [==============================] - 0s 318us/sample - loss: 0.4202 - accuracy: 0.9448\n",
      "Epoch 233/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 0.4161 - accuracy: 0.9448\n",
      "Epoch 234/500\n",
      "453/453 [==============================] - 0s 311us/sample - loss: 0.4128 - accuracy: 0.9426\n",
      "Epoch 235/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.4077 - accuracy: 0.9404\n",
      "Epoch 236/500\n",
      "453/453 [==============================] - 0s 379us/sample - loss: 0.4044 - accuracy: 0.9448\n",
      "Epoch 237/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 0.3993 - accuracy: 0.9426\n",
      "Epoch 238/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.3951 - accuracy: 0.9426\n",
      "Epoch 239/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.3964 - accuracy: 0.9404\n",
      "Epoch 240/500\n",
      "453/453 [==============================] - 0s 329us/sample - loss: 0.4006 - accuracy: 0.9448\n",
      "Epoch 241/500\n",
      "453/453 [==============================] - 0s 323us/sample - loss: 0.4080 - accuracy: 0.9404\n",
      "Epoch 242/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.4239 - accuracy: 0.9360\n",
      "Epoch 243/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.4144 - accuracy: 0.9382\n",
      "Epoch 244/500\n",
      "453/453 [==============================] - 0s 321us/sample - loss: 0.4406 - accuracy: 0.9249\n",
      "Epoch 245/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.4170 - accuracy: 0.9316\n",
      "Epoch 246/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.4010 - accuracy: 0.9404\n",
      "Epoch 247/500\n",
      "453/453 [==============================] - 0s 323us/sample - loss: 0.3917 - accuracy: 0.9470\n",
      "Epoch 248/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.3860 - accuracy: 0.9448\n",
      "Epoch 249/500\n",
      "453/453 [==============================] - 0s 313us/sample - loss: 0.3760 - accuracy: 0.9470\n",
      "Epoch 250/500\n",
      "453/453 [==============================] - 0s 323us/sample - loss: 0.3696 - accuracy: 0.9448\n",
      "Epoch 251/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.3726 - accuracy: 0.9404\n",
      "Epoch 252/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.3633 - accuracy: 0.9470\n",
      "Epoch 253/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.3554 - accuracy: 0.9426\n",
      "Epoch 254/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.3479 - accuracy: 0.9404\n",
      "Epoch 255/500\n",
      "453/453 [==============================] - 0s 337us/sample - loss: 0.3445 - accuracy: 0.9404\n",
      "Epoch 256/500\n",
      "453/453 [==============================] - 0s 315us/sample - loss: 0.3411 - accuracy: 0.9448\n",
      "Epoch 257/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.3371 - accuracy: 0.9448\n",
      "Epoch 258/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.3341 - accuracy: 0.9470\n",
      "Epoch 259/500\n",
      "453/453 [==============================] - 0s 325us/sample - loss: 0.3307 - accuracy: 0.9514\n",
      "Epoch 260/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 0.3284 - accuracy: 0.9470\n",
      "Epoch 261/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.3286 - accuracy: 0.9448\n",
      "Epoch 262/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.3230 - accuracy: 0.9514\n",
      "Epoch 263/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 0.3197 - accuracy: 0.9492\n",
      "Epoch 264/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 0.3169 - accuracy: 0.9492\n",
      "Epoch 265/500\n",
      "453/453 [==============================] - 0s 321us/sample - loss: 0.3147 - accuracy: 0.9492\n",
      "Epoch 266/500\n",
      "453/453 [==============================] - 0s 319us/sample - loss: 0.3136 - accuracy: 0.9470\n",
      "Epoch 267/500\n",
      "453/453 [==============================] - 0s 313us/sample - loss: 0.3098 - accuracy: 0.9492\n",
      "Epoch 268/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.3072 - accuracy: 0.9448\n",
      "Epoch 269/500\n",
      "453/453 [==============================] - 0s 312us/sample - loss: 0.3066 - accuracy: 0.9470\n",
      "Epoch 270/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.3053 - accuracy: 0.9492\n",
      "Epoch 271/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.3012 - accuracy: 0.9492\n",
      "Epoch 272/500\n",
      "453/453 [==============================] - 0s 331us/sample - loss: 0.2989 - accuracy: 0.9492\n",
      "Epoch 273/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.2961 - accuracy: 0.9492\n",
      "Epoch 274/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 0.2938 - accuracy: 0.9514\n",
      "Epoch 275/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 0.2960 - accuracy: 0.9470\n",
      "Epoch 276/500\n",
      "453/453 [==============================] - 0s 315us/sample - loss: 0.2913 - accuracy: 0.9492\n",
      "Epoch 277/500\n",
      "453/453 [==============================] - 0s 336us/sample - loss: 0.2884 - accuracy: 0.9492\n",
      "Epoch 278/500\n",
      "453/453 [==============================] - 0s 311us/sample - loss: 0.2864 - accuracy: 0.9514\n",
      "Epoch 279/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.2844 - accuracy: 0.9514\n",
      "Epoch 280/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.2835 - accuracy: 0.9448\n",
      "Epoch 281/500\n",
      "453/453 [==============================] - 0s 330us/sample - loss: 0.2812 - accuracy: 0.9448\n",
      "Epoch 282/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 0.2784 - accuracy: 0.9492\n",
      "Epoch 283/500\n",
      "453/453 [==============================] - 0s 325us/sample - loss: 0.2767 - accuracy: 0.9492\n",
      "Epoch 284/500\n",
      "453/453 [==============================] - 0s 309us/sample - loss: 0.2744 - accuracy: 0.9492\n",
      "Epoch 285/500\n",
      "453/453 [==============================] - 0s 321us/sample - loss: 0.2716 - accuracy: 0.9514\n",
      "Epoch 286/500\n",
      "453/453 [==============================] - 0s 328us/sample - loss: 0.2701 - accuracy: 0.9492\n",
      "Epoch 287/500\n",
      "453/453 [==============================] - 0s 324us/sample - loss: 0.2684 - accuracy: 0.9492\n",
      "Epoch 288/500\n",
      "453/453 [==============================] - 0s 307us/sample - loss: 0.2665 - accuracy: 0.9426\n",
      "Epoch 289/500\n",
      "453/453 [==============================] - 0s 317us/sample - loss: 0.2648 - accuracy: 0.9492\n",
      "Epoch 290/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.2629 - accuracy: 0.9492\n",
      "Epoch 291/500\n",
      "453/453 [==============================] - 0s 320us/sample - loss: 0.2626 - accuracy: 0.9448\n",
      "Epoch 292/500\n",
      "453/453 [==============================] - 0s 324us/sample - loss: 0.2594 - accuracy: 0.9426\n",
      "Epoch 293/500\n",
      "453/453 [==============================] - 0s 332us/sample - loss: 0.2581 - accuracy: 0.9514\n",
      "Epoch 294/500\n",
      "453/453 [==============================] - 0s 323us/sample - loss: 0.2556 - accuracy: 0.9492\n",
      "Epoch 295/500\n",
      "453/453 [==============================] - 0s 333us/sample - loss: 0.2545 - accuracy: 0.9470\n",
      "Epoch 296/500\n",
      "453/453 [==============================] - 0s 326us/sample - loss: 0.2528 - accuracy: 0.9470\n",
      "Epoch 297/500\n",
      "453/453 [==============================] - 0s 316us/sample - loss: 0.2512 - accuracy: 0.9448\n",
      "Epoch 298/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.2523 - accuracy: 0.9470\n",
      "Epoch 299/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.2507 - accuracy: 0.9470\n",
      "Epoch 300/500\n",
      "453/453 [==============================] - 0s 327us/sample - loss: 0.2528 - accuracy: 0.9536\n",
      "Epoch 301/500\n",
      "453/453 [==============================] - 0s 363us/sample - loss: 0.2548 - accuracy: 0.9448\n",
      "Epoch 302/500\n",
      "453/453 [==============================] - 0s 371us/sample - loss: 0.2504 - accuracy: 0.9514\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 344us/sample - loss: 0.2464 - accuracy: 0.9492\n",
      "Epoch 304/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.2432 - accuracy: 0.9470\n",
      "Epoch 305/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 0.2400 - accuracy: 0.9470\n",
      "Epoch 306/500\n",
      "453/453 [==============================] - 0s 350us/sample - loss: 0.2379 - accuracy: 0.9448\n",
      "Epoch 307/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.2372 - accuracy: 0.9470\n",
      "Epoch 308/500\n",
      "453/453 [==============================] - 0s 407us/sample - loss: 0.2350 - accuracy: 0.9470\n",
      "Epoch 309/500\n",
      "453/453 [==============================] - 0s 380us/sample - loss: 0.2349 - accuracy: 0.9492\n",
      "Epoch 310/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.2326 - accuracy: 0.9492\n",
      "Epoch 311/500\n",
      "453/453 [==============================] - 0s 383us/sample - loss: 0.2312 - accuracy: 0.9448\n",
      "Epoch 312/500\n",
      "453/453 [==============================] - 0s 372us/sample - loss: 0.2291 - accuracy: 0.9514 - loss: 0.2402 - accuracy: 0.\n",
      "Epoch 313/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 0.2303 - accuracy: 0.9492 - loss: 0.2229 - accuracy: 0.\n",
      "Epoch 314/500\n",
      "453/453 [==============================] - 0s 364us/sample - loss: 0.2281 - accuracy: 0.9492\n",
      "Epoch 315/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.2259 - accuracy: 0.9514\n",
      "Epoch 316/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.2246 - accuracy: 0.9470\n",
      "Epoch 317/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.2227 - accuracy: 0.9470\n",
      "Epoch 318/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 0.2214 - accuracy: 0.9426\n",
      "Epoch 319/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.2196 - accuracy: 0.9492\n",
      "Epoch 320/500\n",
      "453/453 [==============================] - 0s 368us/sample - loss: 0.2183 - accuracy: 0.9470\n",
      "Epoch 321/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 0.2178 - accuracy: 0.9470\n",
      "Epoch 322/500\n",
      "453/453 [==============================] - 0s 349us/sample - loss: 0.2177 - accuracy: 0.9492\n",
      "Epoch 323/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.2159 - accuracy: 0.9448\n",
      "Epoch 324/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.2141 - accuracy: 0.9492\n",
      "Epoch 325/500\n",
      "453/453 [==============================] - 0s 353us/sample - loss: 0.2129 - accuracy: 0.9470\n",
      "Epoch 326/500\n",
      "453/453 [==============================] - 0s 339us/sample - loss: 0.2109 - accuracy: 0.9536\n",
      "Epoch 327/500\n",
      "453/453 [==============================] - 0s 363us/sample - loss: 0.2094 - accuracy: 0.9514\n",
      "Epoch 328/500\n",
      "453/453 [==============================] - 0s 368us/sample - loss: 0.2081 - accuracy: 0.9514\n",
      "Epoch 329/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.2085 - accuracy: 0.9514\n",
      "Epoch 330/500\n",
      "453/453 [==============================] - 0s 353us/sample - loss: 0.2066 - accuracy: 0.9536\n",
      "Epoch 331/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.2058 - accuracy: 0.9514\n",
      "Epoch 332/500\n",
      "453/453 [==============================] - 0s 363us/sample - loss: 0.2056 - accuracy: 0.9514\n",
      "Epoch 333/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.2048 - accuracy: 0.9470\n",
      "Epoch 334/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 0.2028 - accuracy: 0.9514\n",
      "Epoch 335/500\n",
      "453/453 [==============================] - 0s 374us/sample - loss: 0.2017 - accuracy: 0.9514\n",
      "Epoch 336/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.1995 - accuracy: 0.9492\n",
      "Epoch 337/500\n",
      "453/453 [==============================] - 0s 348us/sample - loss: 0.2000 - accuracy: 0.9470\n",
      "Epoch 338/500\n",
      "453/453 [==============================] - 0s 369us/sample - loss: 0.1988 - accuracy: 0.9514\n",
      "Epoch 339/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1969 - accuracy: 0.9514\n",
      "Epoch 340/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.1957 - accuracy: 0.9492\n",
      "Epoch 341/500\n",
      "453/453 [==============================] - 0s 363us/sample - loss: 0.1947 - accuracy: 0.9514\n",
      "Epoch 342/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 0.1932 - accuracy: 0.9492\n",
      "Epoch 343/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 0.1926 - accuracy: 0.9514\n",
      "Epoch 344/500\n",
      "453/453 [==============================] - 0s 369us/sample - loss: 0.1922 - accuracy: 0.9470\n",
      "Epoch 345/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 0.1899 - accuracy: 0.9536\n",
      "Epoch 346/500\n",
      "453/453 [==============================] - 0s 350us/sample - loss: 0.1893 - accuracy: 0.9492\n",
      "Epoch 347/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1884 - accuracy: 0.9448\n",
      "Epoch 348/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 0.1875 - accuracy: 0.9470\n",
      "Epoch 349/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1867 - accuracy: 0.9492\n",
      "Epoch 350/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1857 - accuracy: 0.9514\n",
      "Epoch 351/500\n",
      "453/453 [==============================] - 0s 373us/sample - loss: 0.1864 - accuracy: 0.9514\n",
      "Epoch 352/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 0.1844 - accuracy: 0.9514\n",
      "Epoch 353/500\n",
      "453/453 [==============================] - 0s 344us/sample - loss: 0.1846 - accuracy: 0.9514\n",
      "Epoch 354/500\n",
      "453/453 [==============================] - 0s 380us/sample - loss: 0.1845 - accuracy: 0.9492\n",
      "Epoch 355/500\n",
      "453/453 [==============================] - 0s 353us/sample - loss: 0.1817 - accuracy: 0.9514\n",
      "Epoch 356/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1813 - accuracy: 0.9470\n",
      "Epoch 357/500\n",
      "453/453 [==============================] - 0s 352us/sample - loss: 0.1856 - accuracy: 0.9514\n",
      "Epoch 358/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1883 - accuracy: 0.9448\n",
      "Epoch 359/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 0.1905 - accuracy: 0.9470\n",
      "Epoch 360/500\n",
      "453/453 [==============================] - 0s 364us/sample - loss: 0.1910 - accuracy: 0.9426\n",
      "Epoch 361/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1975 - accuracy: 0.9426\n",
      "Epoch 362/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1926 - accuracy: 0.9426\n",
      "Epoch 363/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.1915 - accuracy: 0.9470\n",
      "Epoch 364/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.2074 - accuracy: 0.9382\n",
      "Epoch 365/500\n",
      "453/453 [==============================] - 0s 340us/sample - loss: 0.1980 - accuracy: 0.9470\n",
      "Epoch 366/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1938 - accuracy: 0.9448\n",
      "Epoch 367/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1883 - accuracy: 0.9426\n",
      "Epoch 368/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.1993 - accuracy: 0.9426\n",
      "Epoch 369/500\n",
      "453/453 [==============================] - 0s 371us/sample - loss: 0.1945 - accuracy: 0.9492\n",
      "Epoch 370/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1993 - accuracy: 0.9470\n",
      "Epoch 371/500\n",
      "453/453 [==============================] - 0s 343us/sample - loss: 0.1892 - accuracy: 0.9448\n",
      "Epoch 372/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1824 - accuracy: 0.9470\n",
      "Epoch 373/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.1825 - accuracy: 0.9492\n",
      "Epoch 374/500\n",
      "453/453 [==============================] - 0s 350us/sample - loss: 0.1792 - accuracy: 0.9492\n",
      "Epoch 375/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.1762 - accuracy: 0.9514\n",
      "Epoch 376/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1825 - accuracy: 0.9470\n",
      "Epoch 377/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 0.1789 - accuracy: 0.9448\n",
      "Epoch 378/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1741 - accuracy: 0.9470\n",
      "Epoch 379/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1721 - accuracy: 0.9448\n",
      "Epoch 380/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1753 - accuracy: 0.9448\n",
      "Epoch 381/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1688 - accuracy: 0.9514\n",
      "Epoch 382/500\n",
      "453/453 [==============================] - 0s 334us/sample - loss: 0.1684 - accuracy: 0.9470\n",
      "Epoch 383/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1676 - accuracy: 0.9470\n",
      "Epoch 384/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1659 - accuracy: 0.9426\n",
      "Epoch 385/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.1632 - accuracy: 0.9470\n",
      "Epoch 386/500\n",
      "453/453 [==============================] - 0s 341us/sample - loss: 0.1621 - accuracy: 0.9492\n",
      "Epoch 387/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1622 - accuracy: 0.9492\n",
      "Epoch 388/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.1613 - accuracy: 0.9536\n",
      "Epoch 389/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 0.1602 - accuracy: 0.9514\n",
      "Epoch 390/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1596 - accuracy: 0.9514\n",
      "Epoch 391/500\n",
      "453/453 [==============================] - 0s 373us/sample - loss: 0.1583 - accuracy: 0.9536\n",
      "Epoch 392/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1577 - accuracy: 0.9470\n",
      "Epoch 393/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 0.1570 - accuracy: 0.9514\n",
      "Epoch 394/500\n",
      "453/453 [==============================] - 0s 379us/sample - loss: 0.1568 - accuracy: 0.9492\n",
      "Epoch 395/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1548 - accuracy: 0.9536\n",
      "Epoch 396/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.1550 - accuracy: 0.9514\n",
      "Epoch 397/500\n",
      "453/453 [==============================] - 0s 396us/sample - loss: 0.1539 - accuracy: 0.9514\n",
      "Epoch 398/500\n",
      "453/453 [==============================] - 0s 393us/sample - loss: 0.1529 - accuracy: 0.9514\n",
      "Epoch 399/500\n",
      "453/453 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.95 - 0s 412us/sample - loss: 0.1522 - accuracy: 0.9514\n",
      "Epoch 400/500\n",
      "453/453 [==============================] - 0s 404us/sample - loss: 0.1521 - accuracy: 0.9514\n",
      "Epoch 401/500\n",
      "453/453 [==============================] - 0s 434us/sample - loss: 0.1510 - accuracy: 0.9492\n",
      "Epoch 402/500\n",
      "453/453 [==============================] - 0s 416us/sample - loss: 0.1509 - accuracy: 0.9514\n",
      "Epoch 403/500\n",
      "453/453 [==============================] - 0s 424us/sample - loss: 0.1517 - accuracy: 0.9470\n",
      "Epoch 404/500\n",
      "453/453 [==============================] - 0s 408us/sample - loss: 0.1501 - accuracy: 0.9536\n",
      "Epoch 405/500\n",
      "453/453 [==============================] - 0s 368us/sample - loss: 0.1491 - accuracy: 0.9470\n",
      "Epoch 406/500\n",
      "453/453 [==============================] - 0s 384us/sample - loss: 0.1485 - accuracy: 0.9514\n",
      "Epoch 407/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 0.1494 - accuracy: 0.9382\n",
      "Epoch 408/500\n",
      "453/453 [==============================] - 0s 381us/sample - loss: 0.1487 - accuracy: 0.9448\n",
      "Epoch 409/500\n",
      "453/453 [==============================] - 0s 386us/sample - loss: 0.1481 - accuracy: 0.9404\n",
      "Epoch 410/500\n",
      "453/453 [==============================] - 0s 386us/sample - loss: 0.1462 - accuracy: 0.9448\n",
      "Epoch 411/500\n",
      "453/453 [==============================] - 0s 375us/sample - loss: 0.1461 - accuracy: 0.9470\n",
      "Epoch 412/500\n",
      "453/453 [==============================] - 0s 371us/sample - loss: 0.1460 - accuracy: 0.9492\n",
      "Epoch 413/500\n",
      "453/453 [==============================] - 0s 386us/sample - loss: 0.1451 - accuracy: 0.9448\n",
      "Epoch 414/500\n",
      "453/453 [==============================] - 0s 421us/sample - loss: 0.1443 - accuracy: 0.9492\n",
      "Epoch 415/500\n",
      "453/453 [==============================] - 0s 414us/sample - loss: 0.1441 - accuracy: 0.9492\n",
      "Epoch 416/500\n",
      "453/453 [==============================] - 0s 379us/sample - loss: 0.1441 - accuracy: 0.9492\n",
      "Epoch 417/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.1437 - accuracy: 0.9426\n",
      "Epoch 418/500\n",
      "453/453 [==============================] - 0s 369us/sample - loss: 0.1430 - accuracy: 0.9448\n",
      "Epoch 419/500\n",
      "453/453 [==============================] - 0s 369us/sample - loss: 0.1430 - accuracy: 0.9470\n",
      "Epoch 420/500\n",
      "453/453 [==============================] - 0s 372us/sample - loss: 0.1419 - accuracy: 0.9514\n",
      "Epoch 421/500\n",
      "453/453 [==============================] - 0s 372us/sample - loss: 0.1416 - accuracy: 0.9492\n",
      "Epoch 422/500\n",
      "453/453 [==============================] - 0s 391us/sample - loss: 0.1409 - accuracy: 0.9448\n",
      "Epoch 423/500\n",
      "453/453 [==============================] - 0s 436us/sample - loss: 0.1402 - accuracy: 0.9492\n",
      "Epoch 424/500\n",
      "453/453 [==============================] - 0s 438us/sample - loss: 0.1402 - accuracy: 0.9470\n",
      "Epoch 425/500\n",
      "453/453 [==============================] - 0s 402us/sample - loss: 0.1393 - accuracy: 0.9470\n",
      "Epoch 426/500\n",
      "453/453 [==============================] - 0s 390us/sample - loss: 0.1387 - accuracy: 0.9514\n",
      "Epoch 427/500\n",
      "453/453 [==============================] - 0s 368us/sample - loss: 0.1394 - accuracy: 0.9448\n",
      "Epoch 428/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1375 - accuracy: 0.9492\n",
      "Epoch 429/500\n",
      "453/453 [==============================] - 0s 382us/sample - loss: 0.1391 - accuracy: 0.9448\n",
      "Epoch 430/500\n",
      "453/453 [==============================] - 0s 381us/sample - loss: 0.1397 - accuracy: 0.9426\n",
      "Epoch 431/500\n",
      "453/453 [==============================] - 0s 364us/sample - loss: 0.1376 - accuracy: 0.9448\n",
      "Epoch 432/500\n",
      "453/453 [==============================] - 0s 386us/sample - loss: 0.1372 - accuracy: 0.9448 - loss: 0.1470 - accuracy: 0.\n",
      "Epoch 433/500\n",
      "453/453 [==============================] - 0s 379us/sample - loss: 0.1365 - accuracy: 0.9492\n",
      "Epoch 434/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.1355 - accuracy: 0.9492\n",
      "Epoch 435/500\n",
      "453/453 [==============================] - 0s 383us/sample - loss: 0.1356 - accuracy: 0.9382\n",
      "Epoch 436/500\n",
      "453/453 [==============================] - 0s 436us/sample - loss: 0.1354 - accuracy: 0.9492\n",
      "Epoch 437/500\n",
      "453/453 [==============================] - 0s 385us/sample - loss: 0.1348 - accuracy: 0.9470\n",
      "Epoch 438/500\n",
      "453/453 [==============================] - 0s 401us/sample - loss: 0.1336 - accuracy: 0.9448\n",
      "Epoch 439/500\n",
      "453/453 [==============================] - 0s 424us/sample - loss: 0.1333 - accuracy: 0.9492\n",
      "Epoch 440/500\n",
      "453/453 [==============================] - 0s 411us/sample - loss: 0.1328 - accuracy: 0.9426\n",
      "Epoch 441/500\n",
      "453/453 [==============================] - 0s 434us/sample - loss: 0.1332 - accuracy: 0.9470\n",
      "Epoch 442/500\n",
      "453/453 [==============================] - 0s 375us/sample - loss: 0.1324 - accuracy: 0.9448\n",
      "Epoch 443/500\n",
      "453/453 [==============================] - 0s 369us/sample - loss: 0.1321 - accuracy: 0.9514\n",
      "Epoch 444/500\n",
      "453/453 [==============================] - 0s 382us/sample - loss: 0.1315 - accuracy: 0.9470\n",
      "Epoch 445/500\n",
      "453/453 [==============================] - 0s 363us/sample - loss: 0.1317 - accuracy: 0.9470\n",
      "Epoch 446/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1310 - accuracy: 0.9492\n",
      "Epoch 447/500\n",
      "453/453 [==============================] - 0s 370us/sample - loss: 0.1307 - accuracy: 0.9492\n",
      "Epoch 448/500\n",
      "453/453 [==============================] - 0s 369us/sample - loss: 0.1300 - accuracy: 0.9514\n",
      "Epoch 449/500\n",
      "453/453 [==============================] - 0s 430us/sample - loss: 0.1316 - accuracy: 0.9514\n",
      "Epoch 450/500\n",
      "453/453 [==============================] - 0s 426us/sample - loss: 0.1311 - accuracy: 0.9492\n",
      "Epoch 451/500\n",
      "453/453 [==============================] - 0s 387us/sample - loss: 0.1303 - accuracy: 0.9448\n",
      "Epoch 452/500\n",
      "453/453 [==============================] - 0s 387us/sample - loss: 0.1298 - accuracy: 0.9448\n",
      "Epoch 453/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 0.1296 - accuracy: 0.9470\n",
      "Epoch 454/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.1295 - accuracy: 0.9514\n",
      "Epoch 455/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 398us/sample - loss: 0.1286 - accuracy: 0.9492\n",
      "Epoch 456/500\n",
      "453/453 [==============================] - 0s 393us/sample - loss: 0.1291 - accuracy: 0.9536\n",
      "Epoch 457/500\n",
      "453/453 [==============================] - 0s 391us/sample - loss: 0.1280 - accuracy: 0.9514\n",
      "Epoch 458/500\n",
      "453/453 [==============================] - 0s 366us/sample - loss: 0.1277 - accuracy: 0.9492\n",
      "Epoch 459/500\n",
      "453/453 [==============================] - 0s 355us/sample - loss: 0.1266 - accuracy: 0.9514\n",
      "Epoch 460/500\n",
      "453/453 [==============================] - 0s 393us/sample - loss: 0.1268 - accuracy: 0.9448\n",
      "Epoch 461/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.1268 - accuracy: 0.9492\n",
      "Epoch 462/500\n",
      "453/453 [==============================] - 0s 364us/sample - loss: 0.1262 - accuracy: 0.9514\n",
      "Epoch 463/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 0.1263 - accuracy: 0.9470\n",
      "Epoch 464/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1257 - accuracy: 0.9448 - loss: 0.1399 - accuracy: 0.93\n",
      "Epoch 465/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1251 - accuracy: 0.9426\n",
      "Epoch 466/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1249 - accuracy: 0.9492\n",
      "Epoch 467/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.1254 - accuracy: 0.9426\n",
      "Epoch 468/500\n",
      "453/453 [==============================] - 0s 374us/sample - loss: 0.1256 - accuracy: 0.9426\n",
      "Epoch 469/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1245 - accuracy: 0.9492\n",
      "Epoch 470/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1360 - accuracy: 0.9470\n",
      "Epoch 471/500\n",
      "453/453 [==============================] - 0s 348us/sample - loss: 0.1431 - accuracy: 0.9404\n",
      "Epoch 472/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1326 - accuracy: 0.9448\n",
      "Epoch 473/500\n",
      "453/453 [==============================] - 0s 361us/sample - loss: 0.1278 - accuracy: 0.9448\n",
      "Epoch 474/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.1252 - accuracy: 0.9404\n",
      "Epoch 475/500\n",
      "453/453 [==============================] - 0s 375us/sample - loss: 0.1244 - accuracy: 0.9448\n",
      "Epoch 476/500\n",
      "453/453 [==============================] - 0s 413us/sample - loss: 0.1261 - accuracy: 0.9514\n",
      "Epoch 477/500\n",
      "453/453 [==============================] - 0s 372us/sample - loss: 0.1249 - accuracy: 0.9514\n",
      "Epoch 478/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.1231 - accuracy: 0.9492\n",
      "Epoch 479/500\n",
      "453/453 [==============================] - 0s 352us/sample - loss: 0.1236 - accuracy: 0.9514\n",
      "Epoch 480/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.1232 - accuracy: 0.9448\n",
      "Epoch 481/500\n",
      "453/453 [==============================] - 0s 368us/sample - loss: 0.1221 - accuracy: 0.9426\n",
      "Epoch 482/500\n",
      "453/453 [==============================] - 0s 364us/sample - loss: 0.1222 - accuracy: 0.9514\n",
      "Epoch 483/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 0.1227 - accuracy: 0.9426\n",
      "Epoch 484/500\n",
      "453/453 [==============================] - 0s 352us/sample - loss: 0.1210 - accuracy: 0.9492\n",
      "Epoch 485/500\n",
      "453/453 [==============================] - 0s 347us/sample - loss: 0.1198 - accuracy: 0.9470\n",
      "Epoch 486/500\n",
      "453/453 [==============================] - 0s 357us/sample - loss: 0.1204 - accuracy: 0.9514\n",
      "Epoch 487/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1204 - accuracy: 0.9426\n",
      "Epoch 488/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1194 - accuracy: 0.9470\n",
      "Epoch 489/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.1192 - accuracy: 0.9448\n",
      "Epoch 490/500\n",
      "453/453 [==============================] - 0s 354us/sample - loss: 0.1197 - accuracy: 0.9492\n",
      "Epoch 491/500\n",
      "453/453 [==============================] - 0s 345us/sample - loss: 0.1187 - accuracy: 0.9492\n",
      "Epoch 492/500\n",
      "453/453 [==============================] - 0s 362us/sample - loss: 0.1176 - accuracy: 0.9492\n",
      "Epoch 493/500\n",
      "453/453 [==============================] - 0s 358us/sample - loss: 0.1175 - accuracy: 0.9448\n",
      "Epoch 494/500\n",
      "453/453 [==============================] - 0s 342us/sample - loss: 0.1176 - accuracy: 0.9514\n",
      "Epoch 495/500\n",
      "453/453 [==============================] - 0s 351us/sample - loss: 0.1204 - accuracy: 0.9514\n",
      "Epoch 496/500\n",
      "453/453 [==============================] - 0s 366us/sample - loss: 0.1191 - accuracy: 0.9492\n",
      "Epoch 497/500\n",
      "453/453 [==============================] - 0s 360us/sample - loss: 0.1176 - accuracy: 0.9492\n",
      "Epoch 498/500\n",
      "453/453 [==============================] - 0s 368us/sample - loss: 0.1194 - accuracy: 0.9492\n",
      "Epoch 499/500\n",
      "453/453 [==============================] - 0s 365us/sample - loss: 0.1187 - accuracy: 0.9426\n",
      "Epoch 500/500\n",
      "453/453 [==============================] - 0s 359us/sample - loss: 0.1170 - accuracy: 0.9514\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=500, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YXGelKThoTT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poeprYK8h-c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laurence went to dublin a grand party friends and a ladies cakes ask ask your ask ask ask your eyes glisten was glisten hall mchugh cakes jeremy lanigan lanigan boys were might ask ask ask ask ask your eyes glisten was glisten hall mchugh cakes jeremy lanigan lanigan boys were might ask ask ask ask ask your eyes glisten was glisten hall mchugh cakes jeremy lanigan lanigan boys were might ask ask ask ask ask your eyes glisten was glisten hall mchugh cakes jeremy lanigan lanigan boys were might ask ask ask ask ask your eyes glisten was glisten hall mchugh cakes jeremy lanigan\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Laurence went to dublin\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Course 3 - Week 4 - Lesson 1 - Notebook.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
